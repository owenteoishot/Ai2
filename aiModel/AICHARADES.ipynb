{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec12dec0-83df-4756-b533-97d46b74b843",
   "metadata": {},
   "source": [
    "# Data Engineering by Perynn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e65dc290-9188-41bd-aa82-113a3b9a6aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved pose_keypoints\\air_guitar_15153663-uhd_3840_2160_60fps.json (650 frames)\n",
      "✅ Saved pose_keypoints\\air_guitar_5592447-hd_1920_1080_24fps.json (230 frames)\n",
      "✅ Saved pose_keypoints\\air_guitar_6197064-uhd_3840_2160_25fps (1).json (419 frames)\n",
      "✅ Saved pose_keypoints\\air_guitar_6197064-uhd_3840_2160_25fps.json (419 frames)\n",
      "✅ Saved pose_keypoints\\air_guitar_7320716-uhd_3840_2160_25fps.json (167 frames)\n",
      "✅ Saved pose_keypoints\\air_guitar_8513869-uhd_3840_2160_25fps.json (290 frames)\n",
      "✅ Saved pose_keypoints\\air_guitar_9056754-uhd_3840_2160_25fps.json (192 frames)\n",
      "✅ Saved pose_keypoints\\air_guitar_9057178-uhd_3840_2160_25fps.json (266 frames)\n",
      "✅ Saved pose_keypoints\\air_guitar_9057179-uhd_3840_2160_25fps.json (242 frames)\n",
      "✅ Saved pose_keypoints\\air_guitar_9057675-uhd_3840_2160_25fps.json (159 frames)\n",
      "✅ Saved pose_keypoints\\air_guitar_WhatsApp Video 2025-09-29 at 18.09.07_625a26e9.json (130 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_4108054-uhd_3840_2160_25fps.json (485 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_4438086-hd_1920_1080_25fps (1).json (186 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_4438086-hd_1920_1080_25fps.json (186 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_4804853-uhd_3840_2160_25fps.json (396 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_4806554-uhd_3840_2160_25fps.json (97 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_4806559-uhd_3840_2160_25fps.json (424 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_5319929-uhd_3840_2160_25fps.json (704 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_5319971-uhd_3840_2160_25fps.json (532 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_6296160-hd_1080_1920_25fps.json (703 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_7187405-hd_1920_1080_24fps.json (284 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_7187515-hd_1080_1920_24fps.json (531 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_uhd_25fps.json (1075 frames)\n",
      "✅ Saved pose_keypoints\\boxing_punches_WhatsApp Video 2025-09-29 at 18.09.07_64087d0b.json (133 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_3700029-uhd_2160_3840_30fps.json (629 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_6653826-uhd_2160_3840_25fps.json (94 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_6988812-hd_1080_1920_30fps (1).json (69 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_6988812-hd_1080_1920_30fps.json (69 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_7046619-uhd_3840_2160_25fps.json (953 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_7668242-hd_1080_1920_25fps.json (679 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_7774308-uhd_3840_2160_30fps.json (52 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_7774457-uhd_3840_2160_30fps.json (228 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_7774569-uhd_2160_3840_30fps.json (219 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_7774741-uhd_2160_3840_30fps.json (221 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_7855851-uhd_4096_2160_25fps.json (203 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_8174482-uhd_3840_2160_25fps.json (281 frames)\n",
      "✅ Saved pose_keypoints\\playing_games_WhatsApp Video 2025-09-29 at 18.09.07_b3e87305.json (190 frames)\n",
      "✅ Saved pose_keypoints\\talking_phone_5220455-uhd_3840_2160_30fps.json (420 frames)\n",
      "✅ Saved pose_keypoints\\talking_phone_5520072-hd_1080_1920_30fps.json (556 frames)\n",
      "✅ Saved pose_keypoints\\talking_phone_5989755-uhd_2160_3840_25fps.json (322 frames)\n",
      "✅ Saved pose_keypoints\\talking_phone_7552794-hd_1080_1920_25fps.json (364 frames)\n",
      "✅ Saved pose_keypoints\\talking_phone_7592125-uhd_2160_3744_30fps.json (592 frames)\n",
      "✅ Saved pose_keypoints\\talking_phone_7592158-uhd_4096_1974_30fps.json (1438 frames)\n",
      "✅ Saved pose_keypoints\\talking_phone_7681889-hd_1920_1080_25fps.json (592 frames)\n",
      "✅ Saved pose_keypoints\\talking_phone_7681926-hd_1080_1920_25fps.json (640 frames)\n",
      "✅ Saved pose_keypoints\\talking_phone_8344928-uhd_3840_2160_25fps.json (179 frames)\n",
      "✅ Saved pose_keypoints\\talking_phone_8347247-uhd_2160_3840_25fps.json (290 frames)\n",
      "✅ Saved pose_keypoints\\talking_phone_video.json (188 frames)\n",
      "✅ Saved pose_keypoints\\talking_phone_WhatsApp Video 2025-09-29 at 18.12.54_a2cbd305.json (299 frames)\n"
     ]
    }
   ],
   "source": [
    "# import os, cv2, json\n",
    "# import numpy as np\n",
    "# import mediapipe as mp\n",
    "\n",
    "# mp_pose = mp.solutions.pose\n",
    "\n",
    "# # Normalise keypoints to ignore height/position\n",
    "# def normalize_keypoints(pts):\n",
    "#     left_hip, right_hip = pts[23], pts[24]\n",
    "#     center = (left_hip + right_hip) / 2\n",
    "#     pts -= center\n",
    "#     left_shoulder, right_shoulder = pts[11], pts[12]\n",
    "#     scale = np.linalg.norm(left_shoulder - right_shoulder) + 1e-6\n",
    "#     pts /= scale\n",
    "#     return pts[:, :2]  # only x, y\n",
    "\n",
    "# # Process a single video and save JSON\n",
    "# def process_video(video_path, output_path, label):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     pose = mp_pose.Pose()\n",
    "#     sequence = []\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         result = pose.process(rgb)\n",
    "\n",
    "#         if result.pose_landmarks:\n",
    "#             pts = np.array([[l.x, l.y, l.z] for l in result.pose_landmarks.landmark])\n",
    "#             pts_norm = normalize_keypoints(pts)\n",
    "#             sequence.append(pts_norm.flatten().tolist())\n",
    "\n",
    "#     cap.release()\n",
    "#     pose.close()\n",
    "\n",
    "#     with open(output_path, \"w\") as f:\n",
    "#         json.dump({\"label\": label, \"frames\": sequence}, f, indent=2)\n",
    "#     print(f\"✅ Saved {output_path} ({len(sequence)} frames)\")\n",
    "\n",
    "# # Main loop — process all videos\n",
    "# INPUT_DIR = \"dataset_videos\"\n",
    "# OUTPUT_DIR = \"pose_keypoints\"\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# for label in os.listdir(INPUT_DIR):\n",
    "#     folder = os.path.join(INPUT_DIR, label)\n",
    "#     if not os.path.isdir(folder): continue\n",
    "\n",
    "#     for filename in os.listdir(folder):\n",
    "#         if not filename.endswith(\".mp4\"): continue\n",
    "\n",
    "#         video_path = os.path.join(folder, filename)\n",
    "#         out_path = os.path.join(OUTPUT_DIR, f\"{label}_{filename.replace('.mp4','.json')}\")\n",
    "#         process_video(video_path, out_path, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c7dcab2-3134-4d0a-82b3-dba65ef9ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, json, math, random\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_DIR   = \"dataset_videos\"     # dataset/<label>/*.mp4\n",
    "OUTPUT_DIR  = \"pose_keypoints_aug\" # where JSON windows will be written\n",
    "WINDOW      = 48                   # frames per sample\n",
    "STRIDE      = 24                   # hop between windows\n",
    "AUG_PER_WINDOW = 3                 # how many augmented variants to create per window\n",
    "USE_LABEL_AWARE_NUDGE = True       # helpful for phone_call vs playing_games\n",
    "RANDOM_SEED = 42\n",
    "# ==================================================\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# ---- Left/Right joint pairs (Mediapipe 33) ----\n",
    "LR_PAIRS = [\n",
    "    (11,12), (13,14), (15,16),  # shoulders, elbows, wrists\n",
    "    (23,24), (25,26), (27,28),  # hips, knees, ankles\n",
    "    (5,6), (7,8), (9,10),       # eyes, ears, mouth corners (if used)\n",
    "]\n",
    "LEFT_EAR, RIGHT_EAR = 7, 8\n",
    "LEFT_WRIST, RIGHT_WRIST = 15, 16\n",
    "LEFT_SHOULDER, RIGHT_SHOULDER = 11, 12\n",
    "\n",
    "# -------------- Normalization helpers --------------\n",
    "def normalize_keypoints(pts33x3):\n",
    "    \"\"\"\n",
    "    pts33x3: (33,3) in image-normalized coords from Mediapipe (x,y,z in [0..1] approx)\n",
    "    Center at hip midpoint, scale by shoulder width. Returns (33,2) (x,y only)\n",
    "    \"\"\"\n",
    "    pts = pts33x3.copy()\n",
    "    left_hip, right_hip = pts[23], pts[24]\n",
    "    center = (left_hip + right_hip) / 2.0\n",
    "    pts -= center\n",
    "    left_sh, right_sh = pts[LEFT_SHOULDER], pts[RIGHT_SHOULDER]\n",
    "    scale = np.linalg.norm(left_sh[:2] - right_sh[:2]) + 1e-6\n",
    "    pts[:, :2] /= scale\n",
    "    return pts[:, :2]  # (33,2)\n",
    "\n",
    "# -------------- Augmentations --------------\n",
    "def jitter_pose(seq, sigma=0.01):\n",
    "    # seq: (T,33,2)\n",
    "    return seq + np.random.normal(0, sigma, size=seq.shape)\n",
    "\n",
    "def scale_pose(seq, smin=0.95, smax=1.05):\n",
    "    s = random.uniform(smin, smax)\n",
    "    center = seq.mean(axis=1, keepdims=True)  # (T,1,2)\n",
    "    return (seq - center) * s + center\n",
    "\n",
    "def rotate_pose(seq, deg_range=(-8, 8)):\n",
    "    theta = math.radians(random.uniform(*deg_range))\n",
    "    R = np.array([[math.cos(theta), -math.sin(theta)],\n",
    "                  [math.sin(theta),  math.cos(theta)]], dtype=np.float32)\n",
    "    center = seq.mean(axis=1, keepdims=True)\n",
    "    return (seq - center) @ R.T + center\n",
    "\n",
    "def drop_joints(seq, p=0.05):\n",
    "    # randomly set some joints to NaN (occlusion). Your dataloader should handle NaNs (e.g., zero-fill).\n",
    "    mask = (np.random.rand(*seq.shape[:2]) < p)  # (T,33)\n",
    "    seq_occl = seq.copy()\n",
    "    seq_occl[mask] = np.nan\n",
    "    return seq_occl\n",
    "\n",
    "def flip_pose(seq):\n",
    "    # Horizontal flip in normalized space: x -> -x (after our centering/scaling, symmetric around 0)\n",
    "    seq_f = seq.copy()\n",
    "    seq_f[...,0] *= -1.0\n",
    "    # swap left/right joints\n",
    "    for l, r in LR_PAIRS:\n",
    "        seq_f[:, [l, r], :] = seq_f[:, [r, l], :]\n",
    "    return seq_f\n",
    "\n",
    "def time_warp(seq, min_scale=0.9, max_scale=1.1, target_len=None):\n",
    "    # resample temporally to simulate speed changes\n",
    "    T = seq.shape[0]\n",
    "    scale = random.uniform(min_scale, max_scale)\n",
    "    new_T = max(8, int(T * scale))\n",
    "    idx = np.linspace(0, T-1, new_T)\n",
    "    # linear interpolate per joint & coord\n",
    "    seq_warp = np.empty((new_T, seq.shape[1], seq.shape[2]), dtype=np.float32)\n",
    "    for j in range(seq.shape[1]):\n",
    "        for d in range(seq.shape[2]):\n",
    "            seq_warp[:, j, d] = np.interp(idx, np.arange(T), seq[:, j, d])\n",
    "    if target_len:\n",
    "        seq_warp = time_crop_or_pad(seq_warp, target_len)\n",
    "    return seq_warp\n",
    "\n",
    "def time_crop_or_pad(seq, target_len):\n",
    "    T = seq.shape[0]\n",
    "    if T > target_len:\n",
    "        start = np.random.randint(0, T - target_len + 1)\n",
    "        return seq[start:start+target_len]\n",
    "    elif T < target_len:\n",
    "        pad = target_len - T\n",
    "        left = pad // 2\n",
    "        right = pad - left\n",
    "        left_pad = np.repeat(seq[:1], left, axis=0)\n",
    "        right_pad = np.repeat(seq[-1:], right, axis=0)\n",
    "        return np.concatenate([left_pad, seq, right_pad], axis=0)\n",
    "    return seq\n",
    "\n",
    "# ---- Label-aware nudges to teach discriminative cues ----\n",
    "def nudge_towards_phone_call(seq, strength=0.04):\n",
    "    # bring one wrist toward nearest ear each frame\n",
    "    seq2 = seq.copy()\n",
    "    ear_side = random.choice([(LEFT_EAR, LEFT_WRIST), (RIGHT_EAR, RIGHT_WRIST)])\n",
    "    ear_idx, wrist_idx = ear_side\n",
    "    vec = seq2[:, ear_idx, :] - seq2[:, wrist_idx, :]\n",
    "    seq2[:, wrist_idx, :] += strength * vec\n",
    "    return seq2\n",
    "\n",
    "def nudge_towards_playing_game(seq, strength=0.04):\n",
    "    # bring both wrists toward torso midline between shoulders\n",
    "    seq2 = seq.copy()\n",
    "    mid = 0.5*(seq2[:, LEFT_SHOULDER, :] + seq2[:, RIGHT_SHOULDER, :])  # (T,2)\n",
    "    for w in (LEFT_WRIST, RIGHT_WRIST):\n",
    "        vec = mid - seq2[:, w, :]\n",
    "        seq2[:, w, :] += strength * vec\n",
    "    return seq2\n",
    "\n",
    "def apply_random_aug(seq, label=None):\n",
    "    s = seq.copy()\n",
    "    # random geometric/noise\n",
    "    if random.random() < 0.9: s = jitter_pose(s, sigma=0.01)\n",
    "    if random.random() < 0.5: s = scale_pose(s, 0.95, 1.05)\n",
    "    if random.random() < 0.5: s = rotate_pose(s, (-8, 8))\n",
    "    if random.random() < 0.4: s = drop_joints(s, p=0.05)\n",
    "    if random.random() < 0.5: s = flip_pose(s)\n",
    "    if random.random() < 0.5: s = time_warp(s, 0.9, 1.1, target_len=s.shape[0])\n",
    "\n",
    "    # label-aware cue shaping (optional but helpful)\n",
    "    if USE_LABEL_AWARE_NUDGE and label is not None:\n",
    "        if label.lower() in [\"talking_phone\", \"talking-phone\", \"phone_call\", \"phone-call\"]:\n",
    "            if random.random() < 0.7:\n",
    "                s = nudge_towards_phone_call(s)\n",
    "        if label.lower() in [\"playing_games\", \"playing-games\", \"gaming\", \"play_game\", \"play-game\"]:\n",
    "            if random.random() < 0.7:\n",
    "                s = nudge_towards_playing_game(s)\n",
    "    return s\n",
    "\n",
    "# -------------- Core extraction --------------\n",
    "def extract_sequence_from_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    pose = mp_pose.Pose()\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        res = pose.process(rgb)\n",
    "        if res.pose_landmarks:\n",
    "            pts = np.array([[l.x, l.y, l.z] for l in res.pose_landmarks.landmark], dtype=np.float32) # (33,3)\n",
    "            pts_norm = normalize_keypoints(pts) # (33,2)\n",
    "            frames.append(pts_norm)\n",
    "        else:\n",
    "            # if no detection, append NaNs to keep timing consistent\n",
    "            frames.append(np.full((33,2), np.nan, dtype=np.float32))\n",
    "\n",
    "    cap.release()\n",
    "    pose.close()\n",
    "    if len(frames) == 0: return None\n",
    "    return np.stack(frames, axis=0)  # (T,33,2)\n",
    "\n",
    "def generate_windows(seq, window=48, stride=24):\n",
    "    T = seq.shape[0]\n",
    "    if T < 2: return []\n",
    "    windows = []\n",
    "    for start in range(0, max(1, T - window + 1), stride):\n",
    "        chunk = seq[start:start+window]\n",
    "        if chunk.shape[0] < window:\n",
    "            chunk = time_crop_or_pad(chunk, window)\n",
    "        windows.append(chunk)\n",
    "    return windows\n",
    "\n",
    "def save_window_json(window_arr, out_path, label):\n",
    "    # window_arr: (W,33,2)\n",
    "    payload = {\n",
    "        \"label\": label,\n",
    "        \"frames\": window_arr.reshape(window_arr.shape[0], -1).tolist()  # flatten joints per frame\n",
    "    }\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(payload, f, indent=2)\n",
    "\n",
    "# -------------- Main --------------\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for label in os.listdir(INPUT_DIR):\n",
    "    in_label_dir = os.path.join(INPUT_DIR, label)\n",
    "    if not os.path.isdir(in_label_dir): continue\n",
    "\n",
    "    out_label_dir = os.path.join(OUTPUT_DIR, label)\n",
    "    os.makedirs(out_label_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(in_label_dir):\n",
    "        if not filename.lower().endswith(\".mp4\"): continue\n",
    "        video_path = os.path.join(in_label_dir, filename)\n",
    "        seq = extract_sequence_from_video(video_path)\n",
    "        if seq is None:\n",
    "            print(f\"⚠️ No frames for {video_path}\")\n",
    "            continue\n",
    "\n",
    "        windows = generate_windows(seq, WINDOW, STRIDE)\n",
    "        base = os.path.splitext(filename)[0]\n",
    "\n",
    "        for i, w in enumerate(windows):\n",
    "            # Save original window\n",
    "            out0 = os.path.join(out_label_dir, f\"{base}_win{i:03d}_orig.json\")\n",
    "            save_window_json(w, out0, label)\n",
    "\n",
    "            # Save augmented variants\n",
    "            for k in range(AUG_PER_WINDOW):\n",
    "                w_aug = apply_random_aug(w, label=label)\n",
    "                outk = os.path.join(out_label_dir, f\"{base}_win{i:03d}_aug{k+1}.json\")\n",
    "                save_window_json(w_aug, outk, label)\n",
    "\n",
    "        print(f\"✅ {label}/{filename}: {len(windows)} windows x (1+{AUG_PER_WINDOW}) saved -> {out_label_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac38c1-7b22-41af-bd61-619d2eaa5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mediapipe as mp\n",
    "# mp_pose = mp.solutions.pose\n",
    "# print(\"✅ Mediapipe is working!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a91e3-e687-46da-bf1a-1230bb030806",
   "metadata": {},
   "source": [
    "# Machine Learning by Yee Jing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9b327405-7c60-4b1f-b9a5-b41fe7d30af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2941 JSON files. Example:\n",
      "  pose_keypoints_aug\\WhatsApp Video 2025-09-29 at 18.09.07_64087d0b.json\n",
      "Raw label counts (before filtering):\n",
      "  'talking_phone': 912\n",
      "  'boxing_punches': 880\n",
      "  'playing_games': 664\n",
      "  'air_guitar': 484\n",
      "  'WhatsApp Video 2025': 1\n",
      "\n",
      "Allowed classes: ['WhatsApp_Video_2025', 'air_guitar', 'boxing_punches', 'playing_games', 'talking_phone']\n",
      "\n",
      "✅ NPY windows saved: 2944 (bad json files: 0)\n",
      "Per-class written counts:\n",
      "  WhatsApp_Video_2025: 4\n",
      "  air_guitar: 484\n",
      "  boxing_punches: 880\n",
      "  playing_games: 664\n",
      "  talking_phone: 912\n",
      "train.txt -> 2060 lines\n",
      "val.txt -> 441 lines\n",
      "test.txt -> 443 lines\n",
      "\n",
      "CLASSES: ['WhatsApp_Video_2025', 'air_guitar', 'boxing_punches', 'playing_games', 'talking_phone']\n",
      "in_channels: 66\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# AI Charades — End-to-End: JSON -> NPY -> Train\n",
    "# ===============================================\n",
    "\n",
    "import os, re, json, math, random, pathlib, shutil\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -----------------------------\n",
    "# Config (edit as you like)\n",
    "# -----------------------------\n",
    "SRC_DIR    = \"pose_keypoints_aug\"        # JSONs (possibly in subfolders)\n",
    "OUT_DIR    = \"dataset/sequences\"         # where .npy clips will go: OUT_DIR/<label>/*.npy\n",
    "SPLIT_DIR  = \"splits\"\n",
    "MODEL_DIR  = \"models\"\n",
    "\n",
    "WINDOW     = 48       # frames per clip\n",
    "STRIDE     = 24       # hop between windows\n",
    "TEST_RATIO = 0.15     # 15% test\n",
    "VAL_RATIO  = 0.15     # 15% val\n",
    "BATCH_TRAIN= 64\n",
    "BATCH_EVAL = 128\n",
    "EPOCHS     = 100\n",
    "LR         = 1e-3\n",
    "WD         = 1e-3\n",
    "SEED       = 42\n",
    "\n",
    "# Keep labels simple: letters, digits, _, -\n",
    "LABEL_REGEX = re.compile(r\"^[A-Za-z0-9_-]+$\")\n",
    "\n",
    "# If you want to hard-limit to known classes, put them here; leave as None to auto-discover\n",
    "KNOWN_CLASSES = None  # e.g., [\"talking_phone\", \"playing_games\", \"air_guitar\", \"boxing_punches\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Repro\n",
    "# -----------------------------\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Utils\n",
    "# -----------------------------\n",
    "def ensure_dir(d):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def slug(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9_-]+', '_', s.strip())\n",
    "\n",
    "def is_valid_label(s: str) -> bool:\n",
    "    return bool(s) and bool(LABEL_REGEX.match(s))\n",
    "\n",
    "def get_label_from_json(path: str, fname: str) -> str:\n",
    "    \"\"\"Prefer 'label' inside JSON; else parent folder; else filename-derived.\"\"\"\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, dict) and \"label\" in data and data[\"label\"]:\n",
    "            return str(data[\"label\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    parent = pathlib.Path(path).parent.name\n",
    "    if parent and parent != pathlib.Path(SRC_DIR).name:\n",
    "        return parent\n",
    "\n",
    "    base = os.path.splitext(fname)[0]\n",
    "    if \"-\" in base:\n",
    "        base = base.split(\"-\")[0]\n",
    "    parts = base.split(\"_\")\n",
    "    if parts and parts[-1].isdigit():\n",
    "        base = \"_\".join(parts[:-1])\n",
    "    return base\n",
    "\n",
    "def load_frames_json(path: str) -> np.ndarray:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    frames = data.get(\"frames\", data)  # supports dict or plain list\n",
    "    arr = np.array(frames, dtype=np.float32)  # (T, F)\n",
    "    if arr.ndim != 2:\n",
    "        raise ValueError(f\"{path} expected (T,F), got {arr.shape}\")\n",
    "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return arr\n",
    "\n",
    "def time_crop_or_pad(seq: np.ndarray, target_len: int) -> np.ndarray:\n",
    "    T = len(seq)\n",
    "    if T > target_len:\n",
    "        start = np.random.randint(0, T - target_len + 1)\n",
    "        return seq[start:start+target_len]\n",
    "    elif T < target_len:\n",
    "        pad = target_len - T\n",
    "        left = pad // 2\n",
    "        right = pad - left\n",
    "        left_pad = np.repeat(seq[:1], left, axis=0)\n",
    "        right_pad = np.repeat(seq[-1:], right, axis=0)\n",
    "        return np.concatenate([left_pad, seq, right_pad], axis=0)\n",
    "    return seq\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Discover JSONs + labels\n",
    "# -----------------------------\n",
    "ensure_dir(OUT_DIR)\n",
    "ensure_dir(SPLIT_DIR)\n",
    "ensure_dir(MODEL_DIR)\n",
    "\n",
    "json_paths = []\n",
    "for root, _, files in os.walk(SRC_DIR):\n",
    "    for fn in files:\n",
    "        if fn.lower().endswith(\".json\"):\n",
    "            json_paths.append(os.path.join(root, fn))\n",
    "\n",
    "if not json_paths:\n",
    "    raise SystemExit(f\"❌ No JSON files found under {SRC_DIR}. Check path/structure.\")\n",
    "\n",
    "print(f\"Found {len(json_paths)} JSON files. Example:\\n  {json_paths[0]}\")\n",
    "\n",
    "raw_labels = [ get_label_from_json(p, os.path.basename(p)) for p in json_paths ]\n",
    "label_counter = Counter(raw_labels)\n",
    "print(\"Raw label counts (before filtering):\")\n",
    "for k,v in sorted(label_counter.items(), key=lambda kv: (-kv[1], kv[0])):\n",
    "    print(f\"  {k!r}: {v}\")\n",
    "\n",
    "# Allowed labels\n",
    "if KNOWN_CLASSES is not None:\n",
    "    allowed_labels = sorted([c for c in KNOWN_CLASSES if is_valid_label(c)])\n",
    "else:\n",
    "    allowed_labels = sorted({ slug(l) for l in raw_labels if is_valid_label(slug(l)) })\n",
    "\n",
    "print(\"\\nAllowed classes:\", allowed_labels)\n",
    "if not allowed_labels:\n",
    "    raise SystemExit(\"❌ No allowed classes inferred. Set KNOWN_CLASSES or fix your labels.\")\n",
    "\n",
    "# Prepare class folders freshly (optional: clean only unknown/junk)\n",
    "for d in os.listdir(OUT_DIR):\n",
    "    p = os.path.join(OUT_DIR, d)\n",
    "    if os.path.isdir(p) and d not in allowed_labels:\n",
    "        shutil.rmtree(p)  # remove junk class dirs\n",
    "        print(\"Removed junk dir:\", p)\n",
    "for lbl in allowed_labels:\n",
    "    ensure_dir(os.path.join(OUT_DIR, lbl))\n",
    "\n",
    "# -----------------------------\n",
    "# 2) JSON -> fixed-length NPY\n",
    "# -----------------------------\n",
    "meta = []      # list of (path, label)\n",
    "bad_files = 0\n",
    "per_class_written = Counter()\n",
    "\n",
    "for src_path in json_paths:\n",
    "    fname = os.path.basename(src_path)\n",
    "    raw_lbl = get_label_from_json(src_path, fname)\n",
    "    lbl = slug(raw_lbl)\n",
    "    if lbl not in allowed_labels:\n",
    "        # skip labels with spaces or unknown names like 'WhatsApp Video 2025'\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        X = load_frames_json(src_path)  # (T, F)\n",
    "    except Exception as e:\n",
    "        bad_files += 1\n",
    "        print(f\"⚠️ Failed to load {src_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    T = len(X)\n",
    "    if T == 0:\n",
    "        print(f\"⚠️ Empty frames in {src_path}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Sliding windows\n",
    "    i = 0\n",
    "    base = pathlib.Path(fname).stem\n",
    "    out_cls_dir = os.path.join(OUT_DIR, lbl)\n",
    "\n",
    "    wrote_any = False\n",
    "    while i + WINDOW <= T:\n",
    "        clip = X[i:i+WINDOW]\n",
    "        out_path = os.path.join(out_cls_dir, f\"{base}_t{i:05d}.npy\")\n",
    "        np.save(out_path, clip.astype(np.float32))\n",
    "        meta.append((out_path, lbl))\n",
    "        per_class_written[lbl] += 1\n",
    "        wrote_any = True\n",
    "        i += STRIDE\n",
    "\n",
    "    # If the sequence was shorter than WINDOW or remainder didn’t fit, write one padded window\n",
    "    if not wrote_any:\n",
    "        clip = time_crop_or_pad(X, WINDOW)  # (WINDOW, F)\n",
    "        out_path = os.path.join(out_cls_dir, f\"{base}_t00000.npy\")\n",
    "        np.save(out_path, clip.astype(np.float32))\n",
    "        meta.append((out_path, lbl))\n",
    "        per_class_written[lbl] += 1\n",
    "\n",
    "print(f\"\\n✅ NPY windows saved: {len(meta)} (bad json files: {bad_files})\")\n",
    "print(\"Per-class written counts:\")\n",
    "for k in allowed_labels:\n",
    "    print(f\"  {k}: {per_class_written[k]}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Train/Val/Test Splits (TAB-delimited)\n",
    "# -----------------------------\n",
    "random.shuffle(meta)\n",
    "n = len(meta)\n",
    "n_train = int((1.0 - VAL_RATIO - TEST_RATIO) * n)\n",
    "n_val   = int(VAL_RATIO * n)\n",
    "splits = {\n",
    "    \"train.txt\": meta[:n_train],\n",
    "    \"val.txt\":   meta[n_train:n_train+n_val],\n",
    "    \"test.txt\":  meta[n_train+n_val:]\n",
    "}\n",
    "for name, items in splits.items():\n",
    "    with open(os.path.join(SPLIT_DIR, name), \"w\", encoding=\"utf-8\") as f:\n",
    "        for p,l in items:\n",
    "            f.write(f\"{p}\\t{l}\\n\")\n",
    "    print(f\"{name} -> {len(items)} lines\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Dataset & Loaders\n",
    "# -----------------------------\n",
    "class SeqSet(Dataset):\n",
    "    def __init__(self, list_file):\n",
    "        self.items = []\n",
    "        with open(list_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for ln in f:\n",
    "                ln = ln.rstrip(\"\\n\")\n",
    "                if not ln: continue\n",
    "                path, label = ln.split(\"\\t\")\n",
    "                self.items.append((path, label))\n",
    "        if not self.items:\n",
    "            raise RuntimeError(f\"No items in {list_file}.\")\n",
    "        # classes from file\n",
    "        self.classes = sorted({ l for _, l in self.items })\n",
    "        self.cls2id  = {c:i for i,c in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path, label = self.items[i]\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "        x = np.load(path).astype(np.float32)   # (T, F)\n",
    "        x = torch.from_numpy(x).permute(1, 0)  # -> (F, T)\n",
    "        y = torch.tensor(self.cls2id[label], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "train_txt = os.path.join(SPLIT_DIR, \"train.txt\")\n",
    "val_txt   = os.path.join(SPLIT_DIR, \"val.txt\")\n",
    "test_txt  = os.path.join(SPLIT_DIR, \"test.txt\")\n",
    "\n",
    "train_ds = SeqSet(train_txt)\n",
    "val_ds   = SeqSet(val_txt)\n",
    "test_ds  = SeqSet(test_txt)\n",
    "\n",
    "CLASSES  = train_ds.classes\n",
    "CLS2ID   = train_ds.cls2id\n",
    "print(\"\\nCLASSES:\", CLASSES)\n",
    "\n",
    "in_channels = train_ds[0][0].shape[0]   # e.g., 66 (33 joints × 2)\n",
    "print(\"in_channels:\", in_channels)\n",
    "\n",
    "# Dataloaders (num_workers=0 is safe on Windows)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True, num_workers=0, drop_last=False)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_EVAL,  shuffle=False, num_workers=0, drop_last=False)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_EVAL,  shuffle=False, num_workers=0, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0face46e-550a-4e30-8b72-0b8f22d81cbd",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3b3aa9fa-9bb3-4c72-8ddf-50f7b3a86ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train 51.12% loss 1.171 | val 64.17% loss 0.929\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 02 | train 69.08% loss 0.911 | val 70.07% loss 0.618\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 03 | train 75.19% loss 0.678 | val 72.56% loss 0.501\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 04 | train 75.49% loss 0.614 | val 83.67% loss 0.405\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 05 | train 83.20% loss 0.471 | val 86.39% loss 0.324\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 06 | train 86.80% loss 0.363 | val 87.76% loss 0.257\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 07 | train 88.54% loss 0.313 | val 88.89% loss 0.233\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 08 | train 91.36% loss 0.252 | val 88.44% loss 0.242\n",
      "epoch 09 | train 91.12% loss 0.238 | val 93.20% loss 0.149\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 10 | train 92.48% loss 0.201 | val 93.88% loss 0.164\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 11 | train 92.52% loss 0.195 | val 93.42% loss 0.144\n",
      "epoch 12 | train 94.85% loss 0.145 | val 95.46% loss 0.105\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 13 | train 94.90% loss 0.150 | val 95.01% loss 0.121\n",
      "epoch 14 | train 94.61% loss 0.145 | val 94.33% loss 0.150\n",
      "epoch 15 | train 95.97% loss 0.117 | val 96.37% loss 0.104\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 16 | train 96.07% loss 0.102 | val 95.24% loss 0.100\n",
      "epoch 17 | train 95.83% loss 0.115 | val 95.69% loss 0.079\n",
      "epoch 18 | train 96.50% loss 0.098 | val 96.83% loss 0.073\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 19 | train 97.28% loss 0.075 | val 96.37% loss 0.070\n",
      "epoch 20 | train 97.82% loss 0.073 | val 97.51% loss 0.061\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 21 | train 94.61% loss 0.151 | val 96.60% loss 0.100\n",
      "epoch 22 | train 97.23% loss 0.077 | val 95.01% loss 0.118\n",
      "epoch 23 | train 97.09% loss 0.072 | val 95.69% loss 0.075\n",
      "epoch 24 | train 97.04% loss 0.067 | val 97.28% loss 0.049\n",
      "epoch 25 | train 98.30% loss 0.048 | val 97.28% loss 0.062\n",
      "epoch 26 | train 97.96% loss 0.052 | val 98.64% loss 0.046\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 27 | train 95.97% loss 0.097 | val 97.73% loss 0.062\n",
      "epoch 28 | train 97.09% loss 0.087 | val 97.28% loss 0.070\n",
      "epoch 29 | train 97.86% loss 0.059 | val 98.87% loss 0.051\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 30 | train 98.11% loss 0.047 | val 97.73% loss 0.043\n",
      "epoch 31 | train 99.17% loss 0.031 | val 99.32% loss 0.028\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 32 | train 97.72% loss 0.055 | val 97.28% loss 0.051\n",
      "epoch 33 | train 98.59% loss 0.039 | val 98.41% loss 0.035\n",
      "epoch 34 | train 99.27% loss 0.026 | val 98.64% loss 0.034\n",
      "epoch 35 | train 97.82% loss 0.064 | val 95.24% loss 0.077\n",
      "epoch 36 | train 97.62% loss 0.062 | val 98.19% loss 0.047\n",
      "epoch 37 | train 99.08% loss 0.030 | val 98.64% loss 0.030\n",
      "epoch 38 | train 99.17% loss 0.020 | val 98.87% loss 0.030\n",
      "epoch 39 | train 99.08% loss 0.027 | val 97.05% loss 0.071\n",
      "epoch 40 | train 99.22% loss 0.023 | val 97.96% loss 0.033\n",
      "epoch 41 | train 99.22% loss 0.021 | val 97.73% loss 0.050\n",
      "epoch 42 | train 99.22% loss 0.021 | val 99.32% loss 0.035\n",
      "epoch 43 | train 98.88% loss 0.026 | val 97.96% loss 0.030\n",
      "epoch 44 | train 98.25% loss 0.038 | val 97.96% loss 0.044\n",
      "epoch 45 | train 98.93% loss 0.025 | val 98.87% loss 0.039\n",
      "epoch 46 | train 99.51% loss 0.017 | val 99.32% loss 0.042\n",
      "epoch 47 | train 99.71% loss 0.012 | val 99.09% loss 0.020\n",
      "epoch 48 | train 99.76% loss 0.009 | val 99.32% loss 0.025\n",
      "epoch 49 | train 99.90% loss 0.008 | val 98.87% loss 0.028\n",
      "epoch 50 | train 99.76% loss 0.010 | val 99.32% loss 0.024\n",
      "epoch 51 | train 99.81% loss 0.013 | val 98.19% loss 0.046\n",
      "epoch 52 | train 99.51% loss 0.013 | val 97.96% loss 0.056\n",
      "epoch 53 | train 99.81% loss 0.009 | val 99.32% loss 0.032\n",
      "epoch 54 | train 99.81% loss 0.009 | val 99.55% loss 0.011\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 55 | train 99.71% loss 0.010 | val 98.87% loss 0.031\n",
      "epoch 56 | train 99.66% loss 0.010 | val 98.87% loss 0.023\n",
      "epoch 57 | train 99.27% loss 0.019 | val 98.64% loss 0.041\n",
      "epoch 58 | train 96.94% loss 0.088 | val 88.66% loss 0.178\n",
      "epoch 59 | train 88.88% loss 0.393 | val 94.10% loss 0.413\n",
      "epoch 60 | train 96.02% loss 0.117 | val 97.05% loss 0.073\n",
      "epoch 61 | train 98.20% loss 0.045 | val 98.64% loss 0.044\n",
      "epoch 62 | train 99.32% loss 0.023 | val 98.87% loss 0.032\n",
      "epoch 63 | train 99.47% loss 0.020 | val 99.09% loss 0.032\n",
      "epoch 64 | train 99.51% loss 0.021 | val 98.87% loss 0.033\n",
      "epoch 65 | train 99.61% loss 0.014 | val 99.09% loss 0.027\n",
      "epoch 66 | train 99.61% loss 0.011 | val 99.09% loss 0.027\n",
      "epoch 67 | train 99.56% loss 0.015 | val 99.32% loss 0.020\n",
      "epoch 68 | train 99.61% loss 0.013 | val 98.87% loss 0.039\n",
      "epoch 69 | train 99.76% loss 0.012 | val 99.09% loss 0.023\n",
      "epoch 70 | train 99.76% loss 0.011 | val 98.87% loss 0.037\n",
      "epoch 71 | train 99.85% loss 0.009 | val 99.55% loss 0.012\n",
      "epoch 72 | train 99.81% loss 0.008 | val 99.55% loss 0.018\n",
      "epoch 73 | train 100.00% loss 0.007 | val 99.55% loss 0.015\n",
      "epoch 74 | train 99.81% loss 0.008 | val 99.55% loss 0.020\n",
      "epoch 75 | train 100.00% loss 0.005 | val 99.32% loss 0.024\n",
      "epoch 76 | train 99.90% loss 0.005 | val 99.55% loss 0.025\n",
      "epoch 77 | train 99.71% loss 0.008 | val 98.64% loss 0.048\n",
      "epoch 78 | train 99.42% loss 0.019 | val 97.96% loss 0.059\n",
      "epoch 79 | train 99.32% loss 0.017 | val 98.87% loss 0.047\n",
      "epoch 80 | train 99.76% loss 0.009 | val 99.09% loss 0.022\n",
      "epoch 81 | train 99.81% loss 0.006 | val 98.87% loss 0.028\n",
      "epoch 82 | train 99.95% loss 0.005 | val 99.55% loss 0.021\n",
      "epoch 83 | train 99.95% loss 0.004 | val 99.09% loss 0.018\n",
      "epoch 84 | train 99.08% loss 0.016 | val 98.41% loss 0.020\n",
      "epoch 85 | train 98.54% loss 0.043 | val 96.83% loss 0.062\n",
      "epoch 86 | train 98.16% loss 0.043 | val 98.19% loss 0.054\n",
      "epoch 87 | train 99.22% loss 0.020 | val 97.73% loss 0.062\n",
      "epoch 88 | train 98.98% loss 0.028 | val 98.64% loss 0.024\n",
      "epoch 89 | train 99.51% loss 0.016 | val 99.32% loss 0.024\n",
      "epoch 90 | train 99.81% loss 0.007 | val 99.55% loss 0.019\n",
      "epoch 91 | train 99.90% loss 0.005 | val 99.77% loss 0.013\n",
      "  ↳ saved best: models\\ai_charades_tcn.pt\n",
      "epoch 92 | train 99.95% loss 0.003 | val 99.77% loss 0.017\n",
      "epoch 93 | train 99.90% loss 0.005 | val 99.55% loss 0.014\n",
      "epoch 94 | train 99.95% loss 0.003 | val 99.55% loss 0.023\n",
      "epoch 95 | train 100.00% loss 0.002 | val 99.32% loss 0.020\n",
      "epoch 96 | train 100.00% loss 0.002 | val 99.55% loss 0.026\n",
      "epoch 97 | train 100.00% loss 0.002 | val 99.55% loss 0.021\n",
      "epoch 98 | train 99.95% loss 0.003 | val 99.55% loss 0.016\n",
      "epoch 99 | train 99.90% loss 0.003 | val 99.32% loss 0.027\n",
      "epoch 100 | train 100.00% loss 0.003 | val 99.55% loss 0.021\n",
      "\n",
      "BEST VAL ACC: 0.9977324263038548\n",
      "TEST ACC    : 99.55% | loss 0.015\n",
      "\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      "Classes: ['WhatsApp_Video_2025', 'air_guitar', 'boxing_punches', 'playing_games', 'talking_phone']\n",
      "[[  1   0   0   0   0]\n",
      " [  0  86   0   0   0]\n",
      " [  0   0 135   0   0]\n",
      " [  0   0   0  83   1]\n",
      " [  0   1   0   0 136]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 5) Model\n",
    "# -----------------------------\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, in_ch, n_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 128, 5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(128, n_classes)\n",
    "    def forward(self, x):  # x: (B,F,T)\n",
    "        z = self.net(x).squeeze(-1)\n",
    "        return self.fc(z)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TCN(in_channels, len(CLASSES)).to(device)\n",
    "\n",
    "# Class weights for imbalance\n",
    "counts = Counter(lbl for _, lbl in train_ds.items)\n",
    "w = torch.tensor([1.0/max(1, counts[c]) for c in CLASSES], dtype=torch.float32)\n",
    "w = (w / w.mean()).to(device)\n",
    "\n",
    "crit = nn.CrossEntropyLoss(weight=w)\n",
    "opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "save_path = os.path.join(MODEL_DIR, \"ai_charades_tcn.pt\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Training & Evaluation\n",
    "# -----------------------------\n",
    "def evaluate(loader):\n",
    "    model.eval(); total=0; correct=0; loss_sum=0.0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = crit(logits, y)\n",
    "            loss_sum += loss.item()*y.size(0)\n",
    "            total += y.numel()\n",
    "            correct += (logits.argmax(1)==y).sum().item()\n",
    "    return (loss_sum/total) if total>0 else 0.0, (correct/total) if total>0 else 0.0\n",
    "\n",
    "best_val = 0.0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train(); n=0; loss_sum=0.0; correct=0\n",
    "    for x,y in train_dl:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = crit(logits, y)\n",
    "        loss.backward(); opt.step()\n",
    "        n += y.size(0)\n",
    "        loss_sum += loss.item()*y.size(0)\n",
    "        correct += (logits.argmax(1)==y).sum().item()\n",
    "    tr_loss = loss_sum / max(1,n)\n",
    "    tr_acc  = correct / max(1,n)\n",
    "\n",
    "    val_loss, val_acc = evaluate(val_dl)\n",
    "    print(f\"epoch {epoch:02d} | train {tr_acc:.2%} loss {tr_loss:.3f} | val {val_acc:.2%} loss {val_loss:.3f}\")\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        torch.save({\"state_dict\": model.state_dict(),\n",
    "                    \"classes\": CLASSES,\n",
    "                    \"in_channels\": in_channels}, save_path)\n",
    "        print(\"  ↳ saved best:\", save_path)\n",
    "\n",
    "# Load best and evaluate on test\n",
    "ckpt = torch.load(save_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "print(\"\\nBEST VAL ACC:\", best_val)\n",
    "\n",
    "test_loss, test_acc = evaluate(test_dl)\n",
    "print(\"TEST ACC    :\", f\"{test_acc:.2%}\", \"| loss\", f\"{test_loss:.3f}\")\n",
    "\n",
    "# Confusion matrix (quick)\n",
    "def confusion(loader):\n",
    "    cm = np.zeros((len(CLASSES), len(CLASSES)), dtype=int)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device)\n",
    "            pred = model(x).argmax(1).cpu().numpy()\n",
    "            y = y.numpy()\n",
    "            for t,p in zip(y,pred):\n",
    "                cm[t,p] += 1\n",
    "    return cm\n",
    "\n",
    "cm = confusion(test_dl)\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(\"Classes:\", CLASSES)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ad7cb-b1a4-4fc3-88f8-c95a6d042179",
   "metadata": {},
   "source": [
    "Temporal Convolutional Network (TCN) style classifier — essentially a 1D CNN over time for action recognition.\n",
    "\n",
    "Family: 1D CNN / Temporal Convolutional Network (shallow).\n",
    "\n",
    "Purpose: sequence classification (pose-based action recognition).\n",
    "\n",
    "Why it works: Convolutions learn local motion patterns, pooling summarizes the sequence, and FC layer maps to labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43134cec-4e48-470d-b09f-2f8934e9c7b0",
   "metadata": {},
   "source": [
    "## Test on my own video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0745898-848e-470a-bcfa-fb42db8874e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['WhatsApp_Video_2025', 'air_guitar', 'boxing_punches', 'playing_games', 'talking_phone']\n",
      "Checkpoint in_channels: 66\n",
      "✅ Pose saved: infer_pose_json\\WhatsApp Video 2025-09-29 at 18.12.54_a2cbd305.json (299 frames)\n",
      "🧩 Windows: 11 -> infer_windows\\WhatsApp Video 2025-09-29 at 18.12.54_a2cbd305\n",
      "🎬 WhatsApp Video 2025-09-29 at 18.12.54_a2cbd305  →  talking_phone  (conf 1.00)  | top3 = [('talking_phone', 0.9988024234771729), ('playing_games', 0.0010144879342988133), ('air_guitar', 0.00015900166181381792)]\n"
     ]
    }
   ],
   "source": [
    "# run_infer_own_video_tcn.py\n",
    "import os, cv2, json, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import mediapipe as mp\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_VID_DIR   = \"yj_own_vid\"         # folder with your test videos\n",
    "INFER_JSON_DIR  = \"infer_pose_json\"    # keep inference JSONs separate\n",
    "WINDOWS_ROOT    = \"infer_windows\"      # windows per-video subfolders\n",
    "\n",
    "CKPT_PATH       = os.path.join(\"models\", \"ai_charades_tcn.pt\")  # <-- baseline TCN checkpoint\n",
    "\n",
    "WINDOW, STRIDE  = 48, 24\n",
    "USE_NORMALIZE   = True   # True if training used hip-center + shoulder-width scale\n",
    "PRINT_WINDOW_PRED = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ==================================================\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# ----------------- Pose Extraction ----------------\n",
    "def normalize_keypoints(pts33x3):\n",
    "    \"\"\"\n",
    "    pts: (33,3) with x,y in [0,1] from MediaPipe.\n",
    "    Center at hip midpoint, scale by shoulder width; return (33,2).\n",
    "    \"\"\"\n",
    "    xy = pts33x3[:, :2].astype(np.float32)\n",
    "    center = (xy[23] + xy[24]) / 2.0\n",
    "    xy = xy - center\n",
    "    scale = np.linalg.norm(xy[11] - xy[12]) + 1e-6\n",
    "    xy = xy / scale\n",
    "    return xy\n",
    "\n",
    "def extract_pose_to_json(video_path, out_json_path):\n",
    "    os.makedirs(os.path.dirname(out_json_path), exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1,\n",
    "                        enable_segmentation=False,\n",
    "                        min_detection_confidence=0.5,\n",
    "                        min_tracking_confidence=0.5)\n",
    "    sequence = []\n",
    "    last_xy = np.zeros((33,2), dtype=np.float32)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        res = pose.process(rgb)\n",
    "\n",
    "        if res.pose_landmarks and len(res.pose_landmarks.landmark) == 33:\n",
    "            pts = np.array([[l.x, l.y, getattr(l, \"z\", 0.0)]\n",
    "                            for l in res.pose_landmarks.landmark], dtype=np.float32)\n",
    "            xy = normalize_keypoints(pts) if USE_NORMALIZE else pts[:, :2]\n",
    "            last_xy = xy.astype(np.float32)\n",
    "\n",
    "        # keep timing continuity (repeat last when not detected)\n",
    "        sequence.append(last_xy.flatten().tolist())\n",
    "\n",
    "    cap.release(); pose.close()\n",
    "\n",
    "    arr = np.nan_to_num(np.array(sequence, dtype=np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    with open(out_json_path, \"w\") as f:\n",
    "        json.dump({\"label\": None, \"frames\": arr.tolist()}, f)\n",
    "    print(f\"✅ Pose saved: {out_json_path} ({len(arr)} frames)\")\n",
    "\n",
    "# ----------------- Windowing ----------------------\n",
    "def make_windows_for_json(in_json_path, out_dir, window=WINDOW, stride=STRIDE):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with open(in_json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    frames = data.get(\"frames\", data)\n",
    "    X = np.array(frames, dtype=np.float32)  # (T, F=66)\n",
    "    T = len(X)\n",
    "    if T == 0:\n",
    "        print(f\"⚠️  No frames in {in_json_path}\")\n",
    "        return []\n",
    "\n",
    "    if T < window:\n",
    "        pad = np.repeat(X[-1][None,:], window-T, axis=0)\n",
    "        X = np.concatenate([X, pad], axis=0)\n",
    "        T = len(X)\n",
    "\n",
    "    paths = []\n",
    "    i = 0\n",
    "    base = os.path.splitext(os.path.basename(in_json_path))[0]\n",
    "    while i + window <= T:\n",
    "        clip = X[i:i+window]\n",
    "        out_path = os.path.join(out_dir, f\"{base}_t{i:05d}.npy\")\n",
    "        np.save(out_path, clip.astype(np.float32))\n",
    "        paths.append(out_path)\n",
    "        i += stride\n",
    "    print(f\"🧩 Windows: {len(paths)} -> {out_dir}\")\n",
    "    return paths\n",
    "\n",
    "# ----------------- Your Baseline TCN ----------------\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, in_ch, n_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(128, n_classes)\n",
    "    def forward(self, x):  # x: (B,F,T)\n",
    "        z = self.net(x).squeeze(-1)\n",
    "        return self.fc(z)\n",
    "\n",
    "def load_model_and_classes(ckpt_path=CKPT_PATH):\n",
    "    ckpt = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    classes = ckpt[\"classes\"]\n",
    "    in_ch = int(ckpt.get(\"in_channels\", 66))  # your baseline saved this\n",
    "    model = TCN(in_ch, len(classes)).to(DEVICE)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    model.eval()\n",
    "    return model, classes, in_ch\n",
    "\n",
    "# ----------------- Prediction ---------------------\n",
    "def load_clip_tensor(npy_path):\n",
    "    # Baseline: NO velocity; just (F,T)\n",
    "    x = torch.from_numpy(np.load(npy_path).astype(np.float32)).permute(1,0)  # (F,T)\n",
    "    return x.unsqueeze(0).to(DEVICE)  # (1,F,T)\n",
    "\n",
    "def predict_video_from_windows(win_dir, model, classes):\n",
    "    npys = sorted(glob.glob(os.path.join(win_dir, \"*.npy\")))\n",
    "    if not npys:\n",
    "        return None, None, None\n",
    "    probs_sum = torch.zeros(len(classes), device=DEVICE)\n",
    "    per_window = []\n",
    "    with torch.no_grad():\n",
    "        for p in npys:\n",
    "            x = load_clip_tensor(p)\n",
    "            logits = model(x)\n",
    "            probs = torch.softmax(logits, dim=1).squeeze(0)  # (C,)\n",
    "            probs_sum += probs\n",
    "            if PRINT_WINDOW_PRED:\n",
    "                wid = os.path.basename(p)\n",
    "                top_prob, top_idx = torch.max(probs, dim=0)\n",
    "                per_window.append((wid, classes[int(top_idx)], float(top_prob)))\n",
    "    probs_mean = probs_sum / len(npys)\n",
    "    topk_prob, topk_idx = torch.topk(probs_mean, k=min(3, len(classes)))\n",
    "    topk = [(classes[int(i)], float(p)) for p,i in zip(topk_prob.tolist(), topk_idx.tolist())]\n",
    "    pred_label, pred_conf = topk[0]\n",
    "    return (pred_label, pred_conf, topk), per_window\n",
    "\n",
    "# ===================== MAIN =======================\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(INFER_JSON_DIR, exist_ok=True)\n",
    "    os.makedirs(WINDOWS_ROOT, exist_ok=True)\n",
    "\n",
    "    # 0) Load model + classes\n",
    "    model, CLASSES, in_ch = load_model_and_classes(CKPT_PATH)\n",
    "    print(\"Classes:\", CLASSES)\n",
    "    print(\"Checkpoint in_channels:\", in_ch)\n",
    "\n",
    "    # 1) Extract pose JSONs for each video in yj_own_vid/\n",
    "    videos = [f for f in os.listdir(INPUT_VID_DIR)\n",
    "              if f.lower().endswith((\".mp4\", \".mov\", \".mkv\", \".avi\"))]\n",
    "    if not videos:\n",
    "        print(f\"❌ No videos found in {INPUT_VID_DIR}\")\n",
    "        raise SystemExit\n",
    "\n",
    "    json_paths = []\n",
    "    for vid in videos:\n",
    "        in_path  = os.path.join(INPUT_VID_DIR, vid)\n",
    "        out_json = os.path.join(INFER_JSON_DIR, os.path.splitext(vid)[0] + \".json\")\n",
    "        extract_pose_to_json(in_path, out_json)\n",
    "        json_paths.append(out_json)\n",
    "\n",
    "    # 2) Make windows per video\n",
    "    win_dirs = []\n",
    "    for jp in json_paths:\n",
    "        base = os.path.splitext(os.path.basename(jp))[0]\n",
    "        out_dir = os.path.join(WINDOWS_ROOT, base)\n",
    "        make_windows_for_json(jp, out_dir, WINDOW, STRIDE)\n",
    "        win_dirs.append(out_dir)\n",
    "\n",
    "    # 3) Predict per video (baseline: no velocity)\n",
    "    for vd, jp in zip(win_dirs, json_paths):\n",
    "        (label, conf, top3), per_window = predict_video_from_windows(vd, model, CLASSES)\n",
    "        video_name = os.path.splitext(os.path.basename(jp))[0]\n",
    "        if label is None:\n",
    "            print(f\"{video_name}: (no windows)\")\n",
    "        else:\n",
    "            print(f\"🎬 {video_name}  →  {label}  (conf {conf:.2f})  | top3 = {top3}\")\n",
    "            if PRINT_WINDOW_PRED and per_window:\n",
    "                print(\"  Per-window top1:\")\n",
    "                for wid, lab, p in per_window:\n",
    "                    print(f\"   - {wid}: {lab} ({p:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e22a8-22ac-44ce-8879-ba2ae3a6a021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
